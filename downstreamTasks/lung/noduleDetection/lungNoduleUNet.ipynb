{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lungNoduleUNet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMEPFhdYdewkD6u4ud1ivdq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "edfbe77a2f5c4fc8aae33e54ea3d6df9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a19475b8c4e54f518a4b83f310ebd7d1",
              "IPY_MODEL_d08abe6fa0f740ec823e0f01815d3a16"
            ],
            "layout": "IPY_MODEL_ab7a73e03b464ad187f11ea5cec64e9a"
          }
        },
        "a19475b8c4e54f518a4b83f310ebd7d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3448de292a94efc98e57503d1a91773",
            "placeholder": "​",
            "style": "IPY_MODEL_2f0a426381c045b88467ea2154c305b0",
            "value": "0.009 MB of 0.009 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "d08abe6fa0f740ec823e0f01815d3a16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a151ded382b4ba1a317b0e28703fb17",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba16d8b06469400dad2e1ae581c6cae6",
            "value": 1
          }
        },
        "ab7a73e03b464ad187f11ea5cec64e9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3448de292a94efc98e57503d1a91773": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f0a426381c045b88467ea2154c305b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a151ded382b4ba1a317b0e28703fb17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba16d8b06469400dad2e1ae581c6cae6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeanSDarcy2001/CISProgrammingAssignments/blob/main/downstreamTasks/lung/noduleDetection/lungNoduleUNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.utils.data import Dataset\n",
        "import logging\n",
        "from os import listdir\n",
        "from os.path import splitext\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import pip\n",
        "import argparse\n",
        "import logging\n",
        "import sys\n",
        "import wandb\n"
      ],
      "metadata": {
        "id": "kq5yWV0qkdy3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Unet components\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "\n",
        "    #might need to add padding here?\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.down_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, bias=False),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.down_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=3, stride=2)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "        # if you have padding issues, see\n",
        "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
        "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return self.sig(x)"
      ],
      "metadata": {
        "id": "vYdupXtKLw_U"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "w6ooV7bFLn0Z"
      },
      "outputs": [],
      "source": [
        "#unet\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        self.inc = DoubleConv(n_channels, 16)\n",
        "        self.down1 = Down(16, 32)\n",
        "        self.down2 = Down(32, 64)\n",
        "        self.down3 = Down(64, 128)\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = Down(128, 256 // factor)\n",
        "        self.up1 = Up(256, 128 // factor, bilinear)\n",
        "        self.up2 = Up(128, 64 // factor, bilinear)\n",
        "        self.up3 = Up(64, 32 // factor, bilinear)\n",
        "        self.up4 = Up(32, 16, bilinear)\n",
        "        self.outc = OutConv(16, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dice loss\n",
        "\n",
        "\n",
        "def dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon=1e-6):\n",
        "    # Average of Dice coefficient for all batches, or for a single mask\n",
        "    assert input.size() == target.size()\n",
        "    if input.dim() == 2 and reduce_batch_first:\n",
        "        raise ValueError(f'Dice: asked to reduce batch but got tensor without batch dimension (shape {input.shape})')\n",
        "\n",
        "    if input.dim() == 2 or reduce_batch_first:\n",
        "        inter = torch.dot(input.reshape(-1), target.reshape(-1))\n",
        "        sets_sum = torch.sum(input) + torch.sum(target)\n",
        "        if sets_sum.item() == 0:\n",
        "            sets_sum = 2 * inter\n",
        "\n",
        "        return (2 * inter + epsilon) / (sets_sum + epsilon)\n",
        "    else:\n",
        "        # compute and average metric for each batch element\n",
        "        dice = 0\n",
        "        for i in range(input.shape[0]):\n",
        "            dice += dice_coeff(input[i, ...], target[i, ...])\n",
        "        return dice / input.shape[0]\n",
        "\n",
        "\n",
        "def multiclass_dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon=1e-6):\n",
        "    # Average of Dice coefficient for all classes\n",
        "    assert input.size() == target.size()\n",
        "    dice = 0\n",
        "    for channel in range(input.shape[1]):\n",
        "        dice += dice_coeff(input[:, channel, ...], target[:, channel, ...], reduce_batch_first, epsilon)\n",
        "\n",
        "    return dice / input.shape[1]\n",
        "\n",
        "\n",
        "def dice_loss(input: Tensor, target: Tensor, multiclass: bool = False):\n",
        "    # Dice loss (objective to minimize) between 0 and 1\n",
        "    assert input.size() == target.size()\n",
        "    fn = multiclass_dice_coeff if multiclass else dice_coeff\n",
        "    return 1 - fn(input, target, reduce_batch_first=True)"
      ],
      "metadata": {
        "id": "aQDC_fWfcwiQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataLoaders\n",
        "\n",
        "\n",
        "class BasicDataset(Dataset):\n",
        "    def __init__(self, images_dir: str, masks_dir: str, scale: float = 1.0, mask_suffix: str = ''):\n",
        "        self.images_dir = Path(images_dir)\n",
        "        self.masks_dir = Path(masks_dir)\n",
        "        assert 0 < scale <= 1, 'Scale must be between 0 and 1'\n",
        "        self.scale = scale\n",
        "        self.mask_suffix = mask_suffix\n",
        "\n",
        "        self.ids = [splitext(file)[0] for file in listdir(images_dir) if not file.startswith('.')]\n",
        "        if not self.ids:\n",
        "            raise RuntimeError(f'No input file found in {images_dir}, make sure you put your images there')\n",
        "        logging.info(f'Creating dataset with {len(self.ids)} examples')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    @classmethod\n",
        "    def preprocess(cls, pil_img, scale, is_mask):\n",
        "        w, h = pil_img.size\n",
        "        newW, newH = int(scale * w), int(scale * h)\n",
        "        assert newW > 0 and newH > 0, 'Scale is too small, resized images would have no pixel'\n",
        "        pil_img = pil_img.resize((newW, newH), resample=Image.NEAREST if is_mask else Image.BICUBIC)\n",
        "        img_ndarray = np.asarray(pil_img)\n",
        "\n",
        "        if img_ndarray.ndim == 2 and not is_mask:\n",
        "            img_ndarray = img_ndarray[np.newaxis, ...]\n",
        "        elif not is_mask:\n",
        "            img_ndarray = img_ndarray.transpose((2, 0, 1))\n",
        "\n",
        "        if not is_mask:\n",
        "            img_ndarray = img_ndarray / 255\n",
        "\n",
        "        return img_ndarray\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, filename):\n",
        "        ext = splitext(filename)[1]\n",
        "        if ext in ['.npz', '.npy']:\n",
        "            return Image.fromarray(np.load(filename))\n",
        "        elif ext in ['.pt', '.pth']:\n",
        "            return Image.fromarray(torch.load(filename).numpy())\n",
        "        else:\n",
        "            return Image.open(filename)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.ids[idx]\n",
        "        mask_file = list(self.masks_dir.glob(name + self.mask_suffix + '.*'))\n",
        "        img_file = list(self.images_dir.glob(name + '.*'))\n",
        "\n",
        "        assert len(mask_file) == 1, f'Either no mask or multiple masks found for the ID {name}: {mask_file}'\n",
        "        assert len(img_file) == 1, f'Either no image or multiple images found for the ID {name}: {img_file}'\n",
        "        mask = self.load(mask_file[0])\n",
        "        img = self.load(img_file[0])\n",
        "\n",
        "        assert img.size == mask.size, \\\n",
        "            'Image and mask {name} should be the same size, but are {img.size} and {mask.size}'\n",
        "\n",
        "        img = self.preprocess(img, self.scale, is_mask=False)\n",
        "        mask = self.preprocess(mask, self.scale, is_mask=True)\n",
        "\n",
        "        return {\n",
        "            'image': torch.as_tensor(img.copy()).float().contiguous(),\n",
        "            'mask': torch.as_tensor(mask.copy()).long().contiguous()\n",
        "        }\n",
        "\n",
        "\n",
        "class CarvanaDataset(BasicDataset):\n",
        "    def __init__(self, images_dir, masks_dir, scale=1):\n",
        "        super().__init__(images_dir, masks_dir, scale, mask_suffix='')"
      ],
      "metadata": {
        "id": "SU1pZOvXi6fO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluation\n",
        "\n",
        "\n",
        "def evaluate(net, dataloader, device):\n",
        "    net.eval()\n",
        "    num_val_batches = len(dataloader)\n",
        "    dice_score = 0\n",
        "\n",
        "    # iterate over the validation set\n",
        "    for batch in tqdm(dataloader, total=num_val_batches, desc='Validation round', unit='batch', leave=False):\n",
        "        image, mask_true = batch['image'], batch['mask']\n",
        "        # move images and labels to correct device and type\n",
        "        image = image.to(device=device, dtype=torch.float32)\n",
        "        mask_true = mask_true.to(device=device, dtype=torch.long)\n",
        "        mask_true = F.one_hot(mask_true, net.n_classes).permute(0, 3, 1, 2).float()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # predict the mask\n",
        "            mask_pred = net(image)\n",
        "\n",
        "            # convert to one-hot format\n",
        "            if net.n_classes == 1:\n",
        "                mask_pred = (F.sigmoid(mask_pred) > 0.5).float()\n",
        "                # compute the Dice score\n",
        "                dice_score += dice_coeff(mask_pred, mask_true, reduce_batch_first=False)\n",
        "            else:\n",
        "                mask_pred = F.one_hot(mask_pred.argmax(dim=1), net.n_classes).permute(0, 3, 1, 2).float()\n",
        "                # compute the Dice score, ignoring background\n",
        "                dice_score += multiclass_dice_coeff(mask_pred[:, 1:, ...], mask_true[:, 1:, ...], reduce_batch_first=False)\n",
        "\n",
        "           \n",
        "\n",
        "    net.train()\n",
        "\n",
        "    # Fixes a potential division by zero error\n",
        "    if num_val_batches == 0:\n",
        "        return dice_score\n",
        "    return dice_score / num_val_batches"
      ],
      "metadata": {
        "id": "kEqldmHajC-Y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: specify paths\n",
        "from google.colab import drive\n",
        "drive.mount('gdrive/')\n",
        "\n",
        "dir_img = Path('gdrive/My Drive/deepDRR_3dseg/images')\n",
        "dir_mask = Path('gdrive/My Drive/deepDRR_3dseg/images')\n",
        "dir_checkpoint = Path('./checkpoints/')\n",
        "\n",
        "\n",
        "def train_net(net,\n",
        "              device,\n",
        "              epochs: int = 400,\n",
        "              batch_size: int = 1,\n",
        "              learning_rate: float = .01,\n",
        "              val_percent: float = 0.2,\n",
        "              save_checkpoint: bool = False,\n",
        "              img_scale: float = 1.0,\n",
        "              amp: bool = False):\n",
        "  \n",
        "    # 1. Create dataset\n",
        "    try:\n",
        "        dataset = CarvanaDataset(dir_img, dir_mask, img_scale)\n",
        "    except (AssertionError, RuntimeError):\n",
        "        dataset = BasicDataset(dir_img, dir_mask, img_scale)\n",
        "\n",
        "    # 2. Split into train / validation partitions\n",
        "    n_val = int(len(dataset) * val_percent)\n",
        "    n_train = len(dataset) - n_val\n",
        "    train_set, val_set = random_split(dataset, [n_train, n_val], generator=torch.Generator().manual_seed(0))\n",
        "\n",
        "    # 3. Create data loaders\n",
        "    loader_args = dict(batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "    train_loader = DataLoader(train_set, shuffle=True, **loader_args)\n",
        "    val_loader = DataLoader(val_set, shuffle=False, drop_last=True, **loader_args)\n",
        "\n",
        "    # (Initialize logging)\n",
        "    experiment = wandb.init(project='U-Net', resume='allow', anonymous='must')\n",
        "    experiment.config.update(dict(epochs=epochs, batch_size=batch_size, learning_rate=learning_rate,\n",
        "                                  val_percent=val_percent, save_checkpoint=save_checkpoint, img_scale=img_scale,\n",
        "                                  amp=amp))\n",
        "\n",
        "    logging.info(f'''Starting training:\n",
        "        Epochs:          {epochs}\n",
        "        Batch size:      {batch_size}\n",
        "        Learning rate:   {learning_rate}\n",
        "        Training size:   {n_train}\n",
        "        Validation size: {n_val}\n",
        "        Checkpoints:     {save_checkpoint}\n",
        "        Device:          {device.type}\n",
        "        Images scaling:  {img_scale}\n",
        "        Mixed Precision: {amp}\n",
        "    ''')\n",
        "\n",
        "    # 4. Set up the optimizer, the loss, the learning rate scheduler and the loss scaling for AMP\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n",
        "    #scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2)  # goal: maximize Dice score\n",
        "    #grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    global_step = 0\n",
        "\n",
        "    # 5. Begin training\n",
        "    for epoch in range(epochs):\n",
        "        net.train()\n",
        "        epoch_loss = 0\n",
        "        with tqdm(total=n_train, desc=f'Epoch {epoch + 1}/{epochs}', unit='img') as pbar:\n",
        "            for batch in train_loader:\n",
        "                images = batch['image']\n",
        "                true_masks = batch['mask']\n",
        "\n",
        "                assert images.shape[1] == net.n_channels, \\\n",
        "                    f'Network has been defined with {net.n_channels} input channels, ' \\\n",
        "                    f'but loaded images have {images.shape[1]} channels. Please check that ' \\\n",
        "                    'the images are loaded correctly.'\n",
        "\n",
        "                images = images.to(device=device, dtype=torch.float32)\n",
        "                true_masks = true_masks.to(device=device, dtype=torch.long)\n",
        "\n",
        "                with torch.cuda.amp.autocast(enabled=amp):\n",
        "                    masks_pred = net(images)\n",
        "                    loss = dice_loss(F.softmax(masks_pred, dim=1).float(),\n",
        "                                       F.one_hot(true_masks, net.n_classes).permute(0, 3, 1, 2).float(),\n",
        "                                       multiclass=True)\n",
        "\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                pbar.update(images.shape[0])\n",
        "                global_step += 1\n",
        "                epoch_loss += loss.item()\n",
        "                experiment.log({\n",
        "                    'train loss': loss.item(),\n",
        "                    'step': global_step,\n",
        "                    'epoch': epoch\n",
        "                })\n",
        "                pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
        "\n",
        "                # Evaluation round\n",
        "                division_step = (n_train // (10 * batch_size))\n",
        "                if division_step > 0:\n",
        "                    if global_step % division_step == 0:\n",
        "                        histograms = {}\n",
        "                        for tag, value in net.named_parameters():\n",
        "                            tag = tag.replace('/', '.')\n",
        "                            histograms['Weights/' + tag] = wandb.Histogram(value.data.cpu())\n",
        "                            histograms['Gradients/' + tag] = wandb.Histogram(value.grad.data.cpu())\n",
        "\n",
        "                        val_score = evaluate(net, val_loader, device)\n",
        "                        #scheduler.step(val_score)\n",
        "\n",
        "                        logging.info('Validation Dice score: {}'.format(val_score))\n",
        "                        experiment.log({\n",
        "                            'learning rate': optimizer.param_groups[0]['lr'],\n",
        "                            'validation Dice': val_score,\n",
        "                            'images': wandb.Image(images[0].cpu()),\n",
        "                            'masks': {\n",
        "                                'true': wandb.Image(true_masks[0].float().cpu()),\n",
        "                                'pred': wandb.Image(torch.softmax(masks_pred, dim=1).argmax(dim=1)[0].float().cpu()),\n",
        "                            },\n",
        "                            'step': global_step,\n",
        "                            'epoch': epoch,\n",
        "                            **histograms\n",
        "                        })\n",
        "\n",
        "        if save_checkpoint:\n",
        "            Path(dir_checkpoint).mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(net.state_dict(), str(dir_checkpoint / 'checkpoint_epoch{}.pth'.format(epoch + 1)))\n",
        "            logging.info(f'Checkpoint {epoch + 1} saved!')\n",
        "\n",
        "\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser(description='Train the UNet on images and target masks')\n",
        "    parser.add_argument('--epochs', '-e', metavar='E', type=int, default=5, help='Number of epochs')\n",
        "    parser.add_argument('--batch-size', '-b', dest='batch_size', metavar='B', type=int, default=1, help='Batch size')\n",
        "    parser.add_argument('--learning-rate', '-l', metavar='LR', type=float, default=1e-5,\n",
        "                        help='Learning rate', dest='lr')\n",
        "    parser.add_argument('--load', '-f', type=str, default=False, help='Load model from a .pth file')\n",
        "    parser.add_argument('--scale', '-s', type=float, default=0.5, help='Downscaling factor of the images')\n",
        "    parser.add_argument('--validation', '-v', dest='val', type=float, default=10.0,\n",
        "                        help='Percent of the data that is used as validation (0-100)')\n",
        "    parser.add_argument('--amp', action='store_true', default=False, help='Use mixed precision')\n",
        "    parser.add_argument('--bilinear', action='store_true', default=False, help='Use bilinear upsampling')\n",
        "\n",
        "    return parser.parse_args()"
      ],
      "metadata": {
        "id": "uqDxz16Yjdbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b2147d7-3f19-4143-b28c-21736880d4fb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at gdrive/; to attempt to forcibly remount, call drive.mount(\"gdrive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args = get_args()\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "logging.info(f'Using device {device}')\n",
        "\n",
        "# Change here to adapt to your data\n",
        "# n_channels=1 for XRay images\n",
        "# n_classes is the number of probabilities you want to get per pixel\n",
        "net = UNet(n_channels=1, n_classes=2, bilinear=args.bilinear)\n",
        "\n",
        "logging.info(f'Network:\\n'\n",
        "              f'\\t{net.n_channels} input channels\\n'\n",
        "              f'\\t{net.n_classes} output channels (classes)\\n'\n",
        "              f'\\t{\"Bilinear\" if net.bilinear else \"Transposed conv\"} upscaling')\n",
        "\n",
        "#if args.load:\n",
        "        #net.load_state_dict(torch.load(args.load, map_location=device))\n",
        "        #logging.info(f'Model loaded from {args.load}')\n",
        "\n",
        "net.to(device=device)\n",
        "try:\n",
        "  train_net(net=net,\n",
        "            epochs=args.epochs,\n",
        "            batch_size=args.batch_size,\n",
        "            learning_rate=args.lr,\n",
        "            device=device,\n",
        "            img_scale=args.scale,\n",
        "            val_percent=args.val / 100,\n",
        "            amp=args.amp)\n",
        "except KeyboardInterrupt:\n",
        "  torch.save(net.state_dict(), 'INTERRUPTED.pth')\n",
        "  logging.info('Saved interrupt')\n",
        "  sys.exit(0)"
      ],
      "metadata": {
        "id": "CMzF2BOcnXdr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 915,
          "referenced_widgets": [
            "edfbe77a2f5c4fc8aae33e54ea3d6df9",
            "a19475b8c4e54f518a4b83f310ebd7d1",
            "d08abe6fa0f740ec823e0f01815d3a16",
            "ab7a73e03b464ad187f11ea5cec64e9a",
            "d3448de292a94efc98e57503d1a91773",
            "2f0a426381c045b88467ea2154c305b0",
            "9a151ded382b4ba1a317b0e28703fb17",
            "ba16d8b06469400dad2e1ae581c6cae6"
          ]
        },
        "outputId": "226fc1be-9d42-45fd-8324-8a0a7f1856c5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: Using device cpu\n",
            "INFO: Network:\n",
            "\t1 input channels\n",
            "\t2 output channels (classes)\n",
            "\tTransposed conv upscaling\n",
            "INFO: Creating dataset with 2 examples\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:1rxu6z42) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "edfbe77a2f5c4fc8aae33e54ea3d6df9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">polar-feather-13</strong>: <a href=\"https://wandb.ai/anony-mouse-250177/U-Net/runs/1rxu6z42?apiKey=5e5f16c82606853d6cee1ff065fac10860fa676a\" target=\"_blank\">https://wandb.ai/anony-mouse-250177/U-Net/runs/1rxu6z42?apiKey=5e5f16c82606853d6cee1ff065fac10860fa676a</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220503_184159-1rxu6z42/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:1rxu6z42). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220503_184241-rp19sumd</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/anony-mouse-250177/U-Net/runs/rp19sumd?apiKey=5e5f16c82606853d6cee1ff065fac10860fa676a\" target=\"_blank\">clean-serenity-14</a></strong> to <a href=\"https://wandb.ai/anony-mouse-250177/U-Net?apiKey=5e5f16c82606853d6cee1ff065fac10860fa676a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: Starting training:\n",
            "        Epochs:          5\n",
            "        Batch size:      1\n",
            "        Learning rate:   1e-05\n",
            "        Training size:   2\n",
            "        Validation size: 0\n",
            "        Checkpoints:     False\n",
            "        Device:          cpu\n",
            "        Images scaling:  0.5\n",
            "        Mixed Precision: False\n",
            "    \n",
            "Epoch 1/5:   0%|          | 0/2 [00:00<?, ?img/s]/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "Epoch 1/5:   0%|          | 0/2 [00:02<?, ?img/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-14564eaccbc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mimg_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mval_percent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             amp=args.amp)\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'INTERRUPTED.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-0edad94de466>\u001b[0m in \u001b[0;36mtrain_net\u001b[0;34m(net, device, epochs, batch_size, learning_rate, val_percent, save_checkpoint, img_scale, amp)\u001b[0m\n\u001b[1;32m     76\u001b[0m                     \u001b[0mmasks_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                     loss = dice_loss(F.softmax(masks_pred, dim=1).float(),\n\u001b[0;32m---> 78\u001b[0;31m                                        \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m                                        multiclass=True)\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Class values must be smaller than num_classes."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# helper Net\n"
      ],
      "metadata": {
        "id": "RioOHK1ChYmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class helpNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
        "        super(helpNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "                      nn.Conv2d(in_channels, 32, kernel_size=3, stride=1, bias=False),\n",
        "                      nn.ReLU(inplace=True))\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "                      nn.Conv2d(32, 32, kernel_size=3, stride=1, bias=False),\n",
        "                      nn.ReLU(inplace=True))\n",
        "        \n",
        "        self.mp = nn.MaxPool2d(2)\n",
        "\n",
        "        self.conv3 = nn.Sequential(\n",
        "                      nn.Conv2d(32, 64, kernel_size=3, stride=1, bias=False),\n",
        "                      nn.ReLU(inplace=True))\n",
        "        \n",
        "        self.conv4 = nn.Sequential(\n",
        "                      nn.Conv2d(64, 64, kernel_size=3, stride=1, bias=False),\n",
        "                      nn.ReLU(inplace=True))\n",
        "        \n",
        "        self.fc1 = nn.Linear(64, 64)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.out = nn.linear(64, self.n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.mp(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.mp(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.out(x)\n",
        "        return x "
      ],
      "metadata": {
        "id": "qyzIUmeohe2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train helper net\n",
        "\n",
        "args = get_args()\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "logging.info(f'Using device {device}')\n",
        "\n",
        "# Change here to adapt to your data\n",
        "# n_channels=1 for XRay images\n",
        "# n_classes is the number of probabilities you want to get per pixel\n",
        "net = helpNet(n_channels=1, n_classes =2, bilinear=args.bilinear)\n",
        "\n",
        "logging.info(f'Network:\\n'\n",
        "              f'\\t{net.n_channels} input channels\\n'\n",
        "              f'\\t{net.n_classes} output channels (classes)\\n'\n",
        "              f'\\t{\"Bilinear\" if net.bilinear else \"Transposed conv\"} upscaling')\n",
        "\n",
        "if args.load:\n",
        "        net.load_state_dict(torch.load(args.load, map_location=device))\n",
        "        logging.info(f'Model loaded from {args.load}')\n",
        "\n",
        "net.to(device=device)\n",
        "try:\n",
        "  train_net(net=net,\n",
        "            epochs=500,\n",
        "            batch_size=32,\n",
        "            learning_rate=.0001)\n",
        "except KeyboardInterrupt:\n",
        "  torch.save(net.state_dict(), 'INTERRUPTED.pth')\n",
        "  logging.info('Saved interrupt')\n",
        "  sys.exit(0)\n",
        "\n",
        "train_net(net, device, epochs = 500,\n",
        "              batch_size: int = 32,\n",
        "              learning_rate: float = .0001,\n",
        "              val_percent: float = 0.2,\n",
        "              save_checkpoint: bool = False,\n",
        "              img_scale: float = 1.0,\n",
        "              amp: bool = False)"
      ],
      "metadata": {
        "id": "MNL1qhNPr7Fi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}